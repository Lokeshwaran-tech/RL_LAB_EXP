{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gymnasium # Retrieve all registered environments\n",
        "envs = gymnasium.envs.registry.values() # Count the total number of\n",
        "total_envs = len(list(envs))\n",
        "print(f\"Total number of environments: {total_envs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw4h9AMzZ9GB",
        "outputId": "5a608f47-1730-4268-a5a3-1ad47fbe0609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of environments: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5yiTCTMZkMn",
        "outputId": "c87049e5-90de-4be7-e2a4-19a11331d507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of environments: 63\n"
          ]
        }
      ],
      "source": [
        "import gymnasium # Retrieve all registered environments\n",
        "envs = gymnasium.envs.registry # Count the total number of\n",
        "total_envs = len(envs)\n",
        "print(f\"Total number of environments: {total_envs}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium # Retrieve all registered environments\n",
        "envs = gymnasium.envs.registry.values() # Print the names of all\n",
        "env_names = sorted([env_spec.id for env_spec in envs])\n",
        "for name in env_names:\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_NMimOZaIGp",
        "outputId": "0fd0b646-4c34-49de-d69f-4b4a989db7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acrobot-v1\n",
            "Ant-v2\n",
            "Ant-v3\n",
            "Ant-v4\n",
            "Ant-v5\n",
            "BipedalWalker-v3\n",
            "BipedalWalkerHardcore-v3\n",
            "Blackjack-v1\n",
            "CarRacing-v3\n",
            "CartPole-v0\n",
            "CartPole-v1\n",
            "CliffWalking-v1\n",
            "CliffWalkingSlippery-v1\n",
            "FrozenLake-v1\n",
            "FrozenLake8x8-v1\n",
            "GymV21Environment-v0\n",
            "GymV26Environment-v0\n",
            "HalfCheetah-v2\n",
            "HalfCheetah-v3\n",
            "HalfCheetah-v4\n",
            "HalfCheetah-v5\n",
            "Hopper-v2\n",
            "Hopper-v3\n",
            "Hopper-v4\n",
            "Hopper-v5\n",
            "Humanoid-v2\n",
            "Humanoid-v3\n",
            "Humanoid-v4\n",
            "Humanoid-v5\n",
            "HumanoidStandup-v2\n",
            "HumanoidStandup-v4\n",
            "HumanoidStandup-v5\n",
            "InvertedDoublePendulum-v2\n",
            "InvertedDoublePendulum-v4\n",
            "InvertedDoublePendulum-v5\n",
            "InvertedPendulum-v2\n",
            "InvertedPendulum-v4\n",
            "InvertedPendulum-v5\n",
            "LunarLander-v3\n",
            "LunarLanderContinuous-v3\n",
            "MountainCar-v0\n",
            "MountainCarContinuous-v0\n",
            "Pendulum-v1\n",
            "Pusher-v2\n",
            "Pusher-v4\n",
            "Pusher-v5\n",
            "Reacher-v2\n",
            "Reacher-v4\n",
            "Reacher-v5\n",
            "Swimmer-v2\n",
            "Swimmer-v3\n",
            "Swimmer-v4\n",
            "Swimmer-v5\n",
            "Taxi-v3\n",
            "Walker2d-v2\n",
            "Walker2d-v3\n",
            "Walker2d-v4\n",
            "Walker2d-v5\n",
            "phys2d/CartPole-v0\n",
            "phys2d/CartPole-v1\n",
            "phys2d/Pendulum-v0\n",
            "tabular/Blackjack-v0\n",
            "tabular/CliffWalking-v0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CartPole MDP Exploration in Jupyter Notebook\n",
        "# Import necessary libraries\n",
        "import gymnasium as gym # Use gymnasium as the maintained successor to gym\n",
        "import numpy as np\n",
        "# Load the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "# Reset the environment to get the initial state\n",
        "# gymnasium's reset returns a tuple (observation, info)\n",
        "state, info = env.reset(seed=42) # Added seed for reproducibility\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "# Define MDP Components for CartPole\n",
        "# State space\n",
        "def describe_state(state):\n",
        "    \"\"\"\n",
        "    This function prints out the individual components of the state\n",
        "    State is a tuple (x, x_dot, theta, theta_dot)\n",
        "    \"\"\"\n",
        "    cart_position, cart_velocity, pole_angle, pole_velocity = state\n",
        "    print(f\"Cart Position: {cart_position}\")\n",
        "    print(f\"Cart Velocity: {cart_velocity}\")\n",
        "    print(f\"Pole Angle: {pole_angle}\")\n",
        "    print(f\"Pole Velocity: {pole_velocity}\")\n",
        "# Example of an initial state in CartPole\n",
        "print(\"Initial State:\")\n",
        "describe_state(state)\n",
        "# Action space exploration\n",
        "# In CartPole, there are two actions: 0 (push left) and 1 (push right)\n",
        "actions = {0: \"Move Left\", 1: \"Move Right\"}\n",
        "for action in actions:\n",
        "    print(f\"Action {action}: {actions[action]}\")\n",
        "# Simulate a few steps to see state transitions and rewards\n",
        "num_steps = 5\n",
        "print(\"\\nSimulating a few steps:\")\n",
        "for step in range(num_steps):\n",
        "    action = env.action_space.sample() # Random action\n",
        "    # gymnasium's step returns (observation, reward, terminated, truncated, info)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated # done is true if either terminated or truncated is true\n",
        "    print(f\"\\nStep {step + 1}:\")\n",
        "    print(f\"Action taken: {actions[action]}\")\n",
        "    print(\"Next State:\")\n",
        "    describe_state(next_state)\n",
        "    print(f\"Reward: {reward}\")\n",
        "    print(f\"Done: {done}\") # Use the combined done flag\n",
        "# Close the environment when done\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1qagf-7aLkI",
        "outputId": "6e4324e4-38dd-4f05-b2a3-867d110f666b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Initial State:\n",
            "Cart Position: 0.02739560417830944\n",
            "Cart Velocity: -0.006112155970185995\n",
            "Pole Angle: 0.03585979342460632\n",
            "Pole Velocity: 0.019736802205443382\n",
            "Action 0: Move Left\n",
            "Action 1: Move Right\n",
            "\n",
            "Simulating a few steps:\n",
            "\n",
            "Step 1:\n",
            "Action taken: Move Left\n",
            "Next State:\n",
            "Cart Position: 0.02727336250245571\n",
            "Cart Velocity: -0.20172953605651855\n",
            "Pole Angle: 0.036254528909921646\n",
            "Pole Velocity: 0.32351475954055786\n",
            "Reward: 1.0\n",
            "Done: False\n",
            "\n",
            "Step 2:\n",
            "Action taken: Move Left\n",
            "Next State:\n",
            "Cart Position: 0.02323877066373825\n",
            "Cart Velocity: -0.39734846353530884\n",
            "Pole Angle: 0.04272482171654701\n",
            "Pole Velocity: 0.6274068355560303\n",
            "Reward: 1.0\n",
            "Done: False\n",
            "\n",
            "Step 3:\n",
            "Action taken: Move Right\n",
            "Next State:\n",
            "Cart Position: 0.015291801653802395\n",
            "Cart Velocity: -0.20284806191921234\n",
            "Pole Angle: 0.05527295917272568\n",
            "Pole Velocity: 0.3484797477722168\n",
            "Reward: 1.0\n",
            "Done: False\n",
            "\n",
            "Step 4:\n",
            "Action taken: Move Left\n",
            "Next State:\n",
            "Cart Position: 0.011234840378165245\n",
            "Cart Velocity: -0.39871081709861755\n",
            "Pole Angle: 0.06224255636334419\n",
            "Pole Velocity: 0.6580671668052673\n",
            "Reward: 1.0\n",
            "Done: False\n",
            "\n",
            "Step 5:\n",
            "Action taken: Move Left\n",
            "Next State:\n",
            "Cart Position: 0.003260623896494508\n",
            "Cart Velocity: -0.594641387462616\n",
            "Pole Angle: 0.07540389895439148\n",
            "Pole Velocity: 0.9696813821792603\n",
            "Reward: 1.0\n",
            "Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can choose between 'FrozenLake-v1' (slippery) and 'FrozenLake8x8-v1' (slippery 8x8)\n",
        "# or 'FrozenLake-v1' with is_slippery=False\n",
        "env_name = 'FrozenLake-v1'\n",
        "env = gym.make(env_name)\n",
        "# Reset the environment to get the initial state\n",
        "state, info = env.reset(seed=42)\n",
        "# Display the action space and observation space\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "# Define MDP Components for FrozenLake\n",
        "# State space: The states in FrozenLake are the grid cells, represented by integers from 0 to N-1, where N is the total number of cells.\n",
        "print(\"\\nState Space:\")\n",
        "print(f\"Number of states: {env.observation_space.n}\")\n",
        "print(\"States represent grid locations.\")\n",
        "# Example: For a 4x4 grid, states are 0 to 15.\n",
        "# Action space: The actions are moving in four directions: Left (0), Down (1), Right (2), Up (3).\n",
        "print(\"\\nAction Space Exploration:\")\n",
        "actions = {0: \"Left\", 1: \"Down\", 2: \"Right\", 3: \"Up\"}\n",
        "for action in actions:\n",
        "    print(f\"Action {action}: {actions[action]}\")\n",
        "# Transition probability and Reward\n",
        "print(\"\\nExploring Transitions and Rewards (for a few steps):\")\n",
        "num_steps = 5\n",
        "env.reset(seed=42) # Reset to a known state for demonstration\n",
        "for step in range(num_steps):\n",
        "    action = env.action_space.sample() # Random action\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    print(f\"\\nStep {step + 1}:\")\n",
        "    print(f\"Action taken: {actions[action]}\")\n",
        "    print(f\"Current State: {state}\") # Print the state *before* the action\n",
        "    print(f\"Next State: {next_state}\")\n",
        "    print(f\"Reward: {reward}\")\n",
        "    print(f\"Done: {done}\")\n",
        "    state = next_state # Update state for the next step\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IctZjkMzaZHY",
        "outputId": "21935ae4-ea21-43da-ec52-69819fd66d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(16)\n",
            "\n",
            "State Space:\n",
            "Number of states: 16\n",
            "States represent grid locations.\n",
            "\n",
            "Action Space Exploration:\n",
            "Action 0: Left\n",
            "Action 1: Down\n",
            "Action 2: Right\n",
            "Action 3: Up\n",
            "\n",
            "Exploring Transitions and Rewards (for a few steps):\n",
            "\n",
            "Step 1:\n",
            "Action taken: Down\n",
            "Current State: 0\n",
            "Next State: 4\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "\n",
            "Step 2:\n",
            "Action taken: Left\n",
            "Current State: 4\n",
            "Next State: 8\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "\n",
            "Step 3:\n",
            "Action taken: Up\n",
            "Current State: 8\n",
            "Next State: 8\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "\n",
            "Step 4:\n",
            "Action taken: Up\n",
            "Current State: 8\n",
            "Next State: 9\n",
            "Reward: 0.0\n",
            "Done: False\n",
            "\n",
            "Step 5:\n",
            "Action taken: Right\n",
            "Current State: 9\n",
            "Next State: 5\n",
            "Reward: 0.0\n",
            "Done: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Load the Blackjack environment\n",
        "# The Blackjack environment in Gymnasium is 'Blackjack-v1'\n",
        "env = gym.make('Blackjack-v1')\n",
        "# Reset the environment\n",
        "# The state in Blackjack is a tuple: (player's current sum, dealer's showing card, whether the player has a usable ace)\n",
        "state, info = env.reset(seed=42)\n",
        "# Display action space and observation space\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "# Define MDP Components for Blackjack\n",
        "# State space: The state is a tuple (player's current sum, dealer's showing card, whether the player has a usable ace)\n",
        "print(\"\\nState Space:\")\n",
        "print(f\"Initial State: {state}\")\n",
        "print(\"State represents (player's current sum, dealer's showing card, whether the player has a usable ace).\")\n",
        "print(\" - Player's current sum: Integer from 4 to 21.\")\n",
        "print(\" - Dealer's showing card: Integer from 1 (Ace) to 10 (10 or face card).\")\n",
        "print(\" - Usable ace: Boolean (True if the player has an ace they can count as 11 without busting, False otherwise).\")\n",
        "# Action space: The actions are discrete: 0 (stick), 1 (hit)\n",
        "print(\"\\nAction Space Exploration:\")\n",
        "actions = {0: \"Stick\", 1: \"Hit\"}\n",
        "for action in actions:\n",
        "    print(f\"Action {action}: {actions[action]}\")\n",
        "print(\"\\nExploring Transitions and Rewards (simulating player actions):\")\n",
        "# Reset to a known state for demonstration, though the initial state is random in Blackjack\n",
        "state, info = env.reset(seed=42)\n",
        "print(f\"Initial State: {state}\")\n",
        "# num_steps_to_simulate is not defined, assuming a small number like 5\n",
        "num_steps_to_simulate = 5\n",
        "for step in range(num_steps_to_simulate):\n",
        "    # In a real scenario, an agent would choose an action based on the state.\n",
        "    # Here, we'll just take a random action (hit or stick).\n",
        "    # Taking 'stick' (action 0) will usually end the episode quickly.\n",
        "    # Let's try taking 'hit' (action 1) for a few steps if possible.\n",
        "    action = 1 # Try to hit\n",
        "    # Check if the action is valid in the current state (always true for hit/stick in Blackjack before episode ends)\n",
        "    if action in [0, 1]:\n",
        "        print(f\"\\nStep {step + 1}:\")\n",
        "        print(f\"Action taken: {actions[action]}\")\n",
        "        print(f\"Current State: {state}\") # Print the state *before* the action\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        print(f\"Next State: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "        print(f\"Done: {done}\")\n",
        "        state = next_state # Update state for the next step\n",
        "        if done:\n",
        "            print(\"Episode finished.\")\n",
        "            break # Stop simulating if the episode is done\n",
        "    else:\n",
        "        print(f\"Invalid action {action} taken.\")\n",
        "        break\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fq2TGoPamuS",
        "outputId": "2b2a10c5-4aa6-4366-c227-f8f5162aacfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "\n",
            "State Space:\n",
            "Initial State: (15, 2, 0)\n",
            "State represents (player's current sum, dealer's showing card, whether the player has a usable ace).\n",
            " - Player's current sum: Integer from 4 to 21.\n",
            " - Dealer's showing card: Integer from 1 (Ace) to 10 (10 or face card).\n",
            " - Usable ace: Boolean (True if the player has an ace they can count as 11 without busting, False otherwise).\n",
            "\n",
            "Action Space Exploration:\n",
            "Action 0: Stick\n",
            "Action 1: Hit\n",
            "\n",
            "Exploring Transitions and Rewards (simulating player actions):\n",
            "Initial State: (15, 2, 0)\n",
            "\n",
            "Step 1:\n",
            "Action taken: Hit\n",
            "Current State: (15, 2, 0)\n",
            "Next State: (25, 2, 0)\n",
            "Reward: -1.0\n",
            "Done: True\n",
            "Episode finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "# Load the Taxi environment\n",
        "# The Taxi environment in Gymnasium is 'Taxi-v3'\n",
        "env = gym.make('Taxi-v3')\n",
        "# Reset the environment\n",
        "# The state in Taxi is a single integer representing the taxi's location,\n",
        "# passenger's location, and destination location.\n",
        "state, info = env.reset(seed=42)\n",
        "# Display action space and observation space\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(\"\\nState Space:\")\n",
        "print(f\"Initial State: {state}\")\n",
        "print(f\"Number of states: {env.observation_space.n}\")\n",
        "print(\"States encode the taxi's position, passenger's current location, and passenger's destination.\")\n",
        "# Action space: The actions are discrete: 0 (South), 1 (North), 2 (East),\n",
        "# 3 (West), 4 (Pickup), 5 (Dropoff).\n",
        "print(\"\\nAction Space Exploration:\")\n",
        "actions = {0: \"South\", 1: \"North\", 2: \"East\", 3: \"West\", 4: \"Pickup\", 5: \"Dropoff\"}\n",
        "for action in actions:\n",
        "    print(f\"Action {action}: {actions[action]}\")\n",
        "# Simulate a few steps to see state transitions and rewards\n",
        "num_steps = 5\n",
        "print(\"\\nExploring Transitions and Rewards (for a few steps):\")\n",
        "state, info = env.reset(seed=42) # Get the initial state after reset\n",
        "print(f\"Initial State: {state}\")\n",
        "for step in range(num_steps):\n",
        "    action = env.action_space.sample() # Random action\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    print(f\"\\nStep {step + 1}:\")\n",
        "    print(f\"Action taken: {actions[action]}\")\n",
        "    print(f\"Current State: {state}\") # Print the state *before* the action\n",
        "    print(f\"Next State: {next_state}\")\n",
        "    print(f\"Reward: {reward}\")\n",
        "    print(f\"Done: {done}\")\n",
        "    state = next_state # Update state for the next step\n",
        "    if done:\n",
        "        print(\"Episode finished.\")\n",
        "        break # Stop simulating if the episode is done\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xg5D-bia4iq",
        "outputId": "8fa956b2-0893-46af-a477-f9ad167494c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(6)\n",
            "Observation Space: Discrete(500)\n",
            "\n",
            "State Space:\n",
            "Initial State: 386\n",
            "Number of states: 500\n",
            "States encode the taxi's position, passenger's current location, and passenger's destination.\n",
            "\n",
            "Action Space Exploration:\n",
            "Action 0: South\n",
            "Action 1: North\n",
            "Action 2: East\n",
            "Action 3: West\n",
            "Action 4: Pickup\n",
            "Action 5: Dropoff\n",
            "\n",
            "Exploring Transitions and Rewards (for a few steps):\n",
            "Initial State: 386\n",
            "\n",
            "Step 1:\n",
            "Action taken: East\n",
            "Current State: 386\n",
            "Next State: 386\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 2:\n",
            "Action taken: Dropoff\n",
            "Current State: 386\n",
            "Next State: 386\n",
            "Reward: -10\n",
            "Done: False\n",
            "\n",
            "Step 3:\n",
            "Action taken: Pickup\n",
            "Current State: 386\n",
            "Next State: 386\n",
            "Reward: -10\n",
            "Done: False\n",
            "\n",
            "Step 4:\n",
            "Action taken: West\n",
            "Current State: 386\n",
            "Next State: 366\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 5:\n",
            "Action taken: East\n",
            "Current State: 366\n",
            "Next State: 386\n",
            "Reward: -1\n",
            "Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "# Load the CliffWalking environment\n",
        "# The CliffWalking environment in Gymnasium is 'CliffWalking-v1'\n",
        "env = gym.make('CliffWalking-v1')\n",
        "# Reset the environment\n",
        "# The state in CliffWalking is a single integer representing the agent's position on the grid.\n",
        "state, info = env.reset(seed=42)\n",
        "# Display action space and observation space\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "# Define MDP Components for CliffWalking\n",
        "# State space: The state is a single integer representing the agent's position on the grid.\n",
        "print(\"\\nState Space:\")\n",
        "print(f\"Initial State: {state}\")\n",
        "print(f\"Number of states: {env.observation_space.n}\")\n",
        "print(\"States represent the agent's position on the grid.\")\n",
        "# Action space: The actions are discrete: 0 (Up), 1 (Right), 2 (Down), 3 (Left).\n",
        "print(\"\\nAction Space Exploration:\")\n",
        "actions = {0: \"Up\", 1: \"Right\", 2: \"Down\", 3: \"Left\"}\n",
        "for action in actions:\n",
        "    print(f\"Action {action}: {actions[action]}\")\n",
        "# Simulate a few steps to see state transitions and rewards\n",
        "num_steps = 5\n",
        "print(\"\\nExploring Transitions and Rewards (for a few steps):\")\n",
        "state, info = env.reset(seed=42) # Get the initial state after reset\n",
        "print(f\"Initial State: {state}\")\n",
        "for step in range(num_steps):\n",
        "    action = env.action_space.sample() # Random action\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    print(f\"\\nStep {step + 1}:\")\n",
        "    print(f\"Action taken: {actions[action]}\")\n",
        "    print(f\"Current State: {state}\") # Print the state *before* the action\n",
        "    print(f\"Next State: {next_state}\")\n",
        "    print(f\"Reward: {reward}\")\n",
        "    print(f\"Done: {done}\")\n",
        "    state = next_state # Update state for the next step\n",
        "    if done:\n",
        "        print(\"Episode finished.\")\n",
        "        break # Stop simulating if the episode is done\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7WjUvCbGdR",
        "outputId": "d61c9fdd-1d6e-4e56-c202-c410eb698dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(48)\n",
            "\n",
            "State Space:\n",
            "Initial State: 36\n",
            "Number of states: 48\n",
            "States represent the agent's position on the grid.\n",
            "\n",
            "Action Space Exploration:\n",
            "Action 0: Up\n",
            "Action 1: Right\n",
            "Action 2: Down\n",
            "Action 3: Left\n",
            "\n",
            "Exploring Transitions and Rewards (for a few steps):\n",
            "Initial State: 36\n",
            "\n",
            "Step 1:\n",
            "Action taken: Up\n",
            "Current State: 36\n",
            "Next State: 24\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 2:\n",
            "Action taken: Up\n",
            "Current State: 24\n",
            "Next State: 12\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 3:\n",
            "Action taken: Up\n",
            "Current State: 12\n",
            "Next State: 0\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 4:\n",
            "Action taken: Up\n",
            "Current State: 0\n",
            "Next State: 0\n",
            "Reward: -1\n",
            "Done: False\n",
            "\n",
            "Step 5:\n",
            "Action taken: Right\n",
            "Current State: 0\n",
            "Next State: 1\n",
            "Reward: -1\n",
            "Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_wYvoBWc4YC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}